{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# InsTAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://store-images.s-microsoft.com/image/apps.31997.13510798887167234.6cd52261-a276-49cf-9b6b-9ef8491fb799.30e70ce4-33c5-43d6-9af1-491fe4679377\" alt=\"Drawing\" style=\"width: 200px;\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Il progetto **InsTAP** ha come obiettivo quello di analizzare diversi profili Instagram e valutare la positività o negatività dei commenti presenti nei vari post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coded by Gianluca Di Franco e Rosario Amantia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Github Project](https://github.com/rosarioamantia/insTAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Immagine del flusso del proggetti*\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sorgente è ovviamente il Social Network Instagram. Per recuperare i dati di cui avevamo bisogno, abbiamo fatto uso di un pacchetto chiamato **Instaloader**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![download.jpg](images/instaloader_ko.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Al fine di generare ed inviare i dati da Instagram abbiamo programmato un producer basato sul pacchetto instaloader citato in precedenza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il producer si occuperà di recuperare ed inviare le seguenti informazioni contenute all'interno dei post degli utenti in esame: \n",
    "- utente che crea il post\n",
    "- Immagine \n",
    "- descrizione\n",
    "- data di creazione\n",
    "- coordinate\n",
    "- commenti "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~pip install instaloader~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/IL2C/instaloader.git@login-fix\n",
      "  Cloning https://github.com/IL2C/instaloader.git (to revision login-fix) to c:\\users\\gianluca\\appdata\\local\\temp\\pip-req-build-8sijk5mx\n",
      "  Resolved https://github.com/IL2C/instaloader.git to commit 8f06668502ca4b90f1e9dff3e954e2146087115e\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.4 in d:\\uni\\tap\\instap\\.venv\\lib\\site-packages (from instaloader==4.9.5) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\uni\\tap\\instap\\.venv\\lib\\site-packages (from requests>=2.4->instaloader==4.9.5) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\uni\\tap\\instap\\.venv\\lib\\site-packages (from requests>=2.4->instaloader==4.9.5) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\uni\\tap\\instap\\.venv\\lib\\site-packages (from requests>=2.4->instaloader==4.9.5) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\uni\\tap\\instap\\.venv\\lib\\site-packages (from requests>=2.4->instaloader==4.9.5) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/IL2C/instaloader.git 'C:\\Users\\Gianluca\\AppData\\Local\\Temp\\pip-req-build-8sijk5mx'\n",
      "  Running command git checkout -b login-fix --track origin/login-fix\n",
      "  Branch 'login-fix' set up to track remote branch 'login-fix' from 'origin'.\n",
      "  Switched to a new branch 'login-fix'\n",
      "WARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'd:\\UNI\\TAP\\insTAP\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install --no-cache-dir git+https://github.com/IL2C/instaloader.git@login-fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"name='csrftoken', domain=None, path=None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minstaloader\u001b[39;00m\n\u001b[0;32m      3\u001b[0m insta \u001b[39m=\u001b[39m instaloader\u001b[39m.\u001b[39mInstaloader()\n\u001b[1;32m----> 4\u001b[0m insta\u001b[39m.\u001b[39;49mlogin(\u001b[39m\"\u001b[39;49m\u001b[39msentimentoanalisi\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mciao12341234\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m posts \u001b[39m=\u001b[39m instaloader\u001b[39m.\u001b[39mProfile\u001b[39m.\u001b[39mfrom_username(insta\u001b[39m.\u001b[39mcontext, \u001b[39m\"\u001b[39m\u001b[39mchiaraferragni\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mget_posts()\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i, post \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(posts):\n",
      "File \u001b[1;32md:\\UNI\\TAP\\insTAP\\.venv\\lib\\site-packages\\instaloader\\instaloader.py:634\u001b[0m, in \u001b[0;36mInstaloader.login\u001b[1;34m(self, user, passwd)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogin\u001b[39m(\u001b[39mself\u001b[39m, user: \u001b[39mstr\u001b[39m, passwd: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39m\"\"\"Log in to instagram with given username and password and internally store session object.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \n\u001b[0;32m    629\u001b[0m \u001b[39m    :raises InvalidArgumentException: If the provided username does not exist.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[39m    :raises TwoFactorAuthRequiredException: First step of 2FA login done, now call\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[39m       :meth:`Instaloader.two_factor_login`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mlogin(user, passwd)\n",
      "File \u001b[1;32md:\\UNI\\TAP\\insTAP\\.venv\\lib\\site-packages\\instaloader\\instaloadercontext.py:265\u001b[0m, in \u001b[0;36mInstaloaderContext.login\u001b[1;34m(self, user, passwd)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidArgumentException(\u001b[39m'\u001b[39m\u001b[39mLogin error: User \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m does not exist.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(user))\n\u001b[0;32m    264\u001b[0m \u001b[39m# '{\"authenticated\": true, \"user\": true, \"userId\": ..., \"oneTapPrompt\": false, \"status\": \"ok\"}'\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m session\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mX-CSRFToken\u001b[39m\u001b[39m'\u001b[39m: login\u001b[39m.\u001b[39;49mcookies[\u001b[39m'\u001b[39;49m\u001b[39mcsrftoken\u001b[39;49m\u001b[39m'\u001b[39;49m]})\n\u001b[0;32m    266\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session \u001b[39m=\u001b[39m session\n\u001b[0;32m    267\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musername \u001b[39m=\u001b[39m user\n",
      "File \u001b[1;32md:\\UNI\\TAP\\insTAP\\.venv\\lib\\site-packages\\requests\\cookies.py:334\u001b[0m, in \u001b[0;36mRequestsCookieJar.__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m    328\u001b[0m     \u001b[39m\"\"\"Dict-like __getitem__() for compatibility with client code. Throws\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[39m    exception if there are more than one cookie with name. In that case,\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[39m    use the more explicit get() method instead.\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \n\u001b[0;32m    332\u001b[0m \u001b[39m    .. warning:: operation is O(n), not O(1).\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_no_duplicates(name)\n",
      "File \u001b[1;32md:\\UNI\\TAP\\insTAP\\.venv\\lib\\site-packages\\requests\\cookies.py:413\u001b[0m, in \u001b[0;36mRequestsCookieJar._find_no_duplicates\u001b[1;34m(self, name, domain, path)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39mif\u001b[39;00m toReturn:\n\u001b[0;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m toReturn\n\u001b[1;32m--> 413\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mname=\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m, domain=\u001b[39m\u001b[39m{\u001b[39;00mdomain\u001b[39m!r}\u001b[39;00m\u001b[39m, path=\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"name='csrftoken', domain=None, path=None\""
     ]
    }
   ],
   "source": [
    "import instaloader\n",
    "\n",
    "insta = instaloader.Instaloader()\n",
    "insta.login(\"user\", \"pass\")\n",
    "posts = instaloader.Profile.from_username(insta.context, \"giorgiameloni\").get_posts()\n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    if i == 3:\n",
    "        break\n",
    "    comments = post.get_comments()\n",
    "    for index, comment in enumerate(comments):\n",
    "        if index == 3:\n",
    "            break\n",
    "\n",
    "        data = {\n",
    "            'message_id': index,\n",
    "            'post_id': f'{post.owner_username}_{i}',\n",
    "            'user': post.owner_username,\n",
    "            'comment': comment.text,\n",
    "            'caption': post.caption,\n",
    "            'image': post.url,\n",
    "            'timestamp': str(post.date_local),\n",
    "            'likes': post.likes,\n",
    "            'lat': post.location.lat if post.location else None,\n",
    "            'lng': post.location.lng if post.location else None\n",
    "        }\n",
    "        print(str(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logstash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![download.jpg](images/logstash.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logstash è uno strumento in grado di raccogliere, modificare e inoltrare dati provenienti da sorgenti diverse verso una o più destinazioni.\n",
    "Il file di configurazione di logstash è formato da tre sezioni:\n",
    "- **input**: ricevere/recuperare le informazioni;\n",
    "- **filter**: rimuovere/aggiungere/modificare campi;\n",
    "- **output**: inviare i dati alla destinazione. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### il nostro .conf\n",
    "```\n",
    "input {\n",
    " http {\n",
    "    port => \"9700\"\n",
    "    host => \"0.0.0.0\"\n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    codec => json\n",
    "    topic_id => \"instap\"\n",
    "    bootstrap_servers => \"http://broker:9092\"\n",
    "  }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel nostro caso, Logstash recupera i dati inviati dalla sorgente alla porta 9700 e li invia al broker di messaggi Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logstash-explained.jpg](images/logstash-explained.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kafka.png](images/kafka.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apache Kafka è una piattaforma distribuita basata sui messaggi e sul modello publish-subscribe con lo scopo di gestire streaming di dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka è ideale per:\n",
    "- Scambi di dati affidabili tra componenti diversi\n",
    "- Gestione dei carichi di lavoro\n",
    "- Streaming in tempo reale per l'elaborazione dei dati\n",
    "- Supporto per la riproduzione di dati/messaggi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concetti chiave di Kafka:\n",
    "- **Producer**: colui che invia i dati. Il producer definisce anche su quale topic dovrà arrivare il messaggio.\n",
    "- **Topic**: argomento del messaggio. \n",
    "    - i producers scelgono su quale topic devono inviare i messaggi;\n",
    "    - i consumers scelgono su quale topic leggere i messaggi. \n",
    "- **Partizione**: struttura dati all'interno del topic in cui vengono scritti i messaggi. Ogni topic ha 1 o più partizioni.\n",
    "- **Consumer**: colui che legge i messaggi. Ogni consumer appartiene ad un gruppo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![consumer-kafka.png](images/consumer-group-kafka.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All'interno del nostro progetto, Kafka è così suddiviso:\n",
    "- **Zookeper**: servizio centralizzatto per il mantenimento di configurazioni e denominazioni; \n",
    "- **Broker**: Cuore di Kafka, gestisce il flusso di dati proveniente da ElasticSearch;\n",
    "- **Init-Kafka**: Script per l'inizializzazione di topic, partition..\n",
    "- **Kafka-Ui**: interfaccia Web relativa a Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3><img src=\"images/spark.png\" width=\"430px\" height=\"226px\"> &nbsp;</h3>\n",
    "<p>Apache Spark è un framework open source di elaborazione parallela che supporta l'elaborazione in memoria per migliorare le prestazioni delle applicazioni che analizzano Big Data.</p>\n",
    "\n",
    "***migliorare descrizione***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>\n",
    "Per l'allenamento del modello è stato utilizzato PySpark ed in particolare la libreria Spark MLlib utilizzando la funzione \"LogisticRegression\" all'interno di una pipeline di machine learning.\n",
    "</p>\n",
    "\n",
    "```\n",
    "#tokenize the Instagram comments text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'comment' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "\n",
    "#remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "\n",
    "#create a word vector of size 50\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 50)\n",
    "\n",
    "#Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'positive')\n",
    "\n",
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
    "\n",
    "dataset = spark.read.csv('../tap/spark/dataset/social_training_set.csv',\n",
    "                         schema=training_set_schema,\n",
    "                         header=True,\n",
    "                         sep=',')\n",
    "\n",
    "#trained model\n",
    "pipelineFit = pipeline.fit(dataset)\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "quando avviene la chiamata pipeline.fit(), viene adattato il modello al training set di input e gli stage vengono eseguiti in ordine per ogni record del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/before_training.jpg\" alighn=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/after_training_1.jpg\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Una volta ottenuto il modello allenato, il passo successivo è quello di leggere i dati inseriti in tempo reale da Kafka nel topic \"instap_topic\".\n",
    "<br>\n",
    "<br>\n",
    "Per leggere il flusso di dati viene usata la libreria Spark Structured Streaming, una libreria per processare i dati in streaming all'interno della libreria Spark SQL\n",
    "<br>\n",
    "\n",
    "```\n",
    "\n",
    "#Create DataFrame representing the stream of input lines from Kafka\n",
    "df = spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', kafkaServer) \\\n",
    "        .option('subscribe', \"instap_topic\"). \\\n",
    "            option(\"startingOffsets\",\"earliest\").load()\n",
    "```\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/kafka_spark.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>\n",
    "   A questo punto, tramite un Output Structured Streaming, si elaborano i dati in stream applicando il modello sui dati di ogni micro-batch ed effettuando una predizione sul commento che verrà aggiunta in una colonna \"prediction\" nel DataFrame<br>\n",
    "</p>\n",
    "\n",
    "```\n",
    "df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n",
    "```\n",
    "<br>\n",
    "\n",
    "<img src=\"images/spark_ml.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una volta processati e analizzati, i dati vengono inviati da PySpark verso Elasticsearch all'interno dell'index \"instap_index\" tramite il client Python\n",
    "\n",
    "```\n",
    "es = Elasticsearch(hosts=elastic_host, verify_certs = False)\n",
    "resp = es.index(index = \"instap_index\", id = id, document = alayzed_data)\n",
    "\n",
    "```\n",
    "<br>\n",
    "<img src=\"images/spark_es.png\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/elasticsearch.png\" align=\"center\" width=\"500px\" height=\"500px\">\n",
    "<p>\n",
    "    Elasticsearch è un motore per la ricerca e l'analisi di dati: è in grado di gestire tutte le tipologie di dato (testuale, numerico, geospaziale, strutturato e non strutturato) ed è conosciuto per la sua natura distribuita, velocità e scalabilità.\n",
    "<br>\n",
    "<br>\n",
    "Elasticsearch è stato utilizzato per indicizzare e memorizzare i dati precedentemente elaborati e inviarli a Kibana per visualizzarli.\n",
    "</p>\n",
    "<br>\n",
    "<b>Nota: </b>una volta memorizzati, i dati  possono essere recuperati in modo veloce ed efficiente attraverso semplici chiamate REST o le apposite API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p><b>Le conferme che rendono felici<b></p>\n",
    "<img src=\"images/es_log.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/kibana.png\" width=\"430px\" height=\"226px\">\n",
    "\n",
    "Kibana è una UI utilizzata per visualizzare ed analizzare i dati di Elasticsearch con l'ausilio di grafici. <br>\n",
    "Di seguito vengono riportati alcuni grafici mostrati nella dashboard. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Homer-Kibana.jpg](images/Homer-kibana.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kibana-dashboard.jpg](images/all-dashboard.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kibana-post-prediction.jpg](images/kibana-post-prediction.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kibana-donut.jpg](images/kibana-donut.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "48a4a3db1827b9653c65b873d88ab4a1a166ed0731694d015661779b08759ad9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
