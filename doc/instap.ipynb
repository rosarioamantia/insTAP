{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# InsTAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://store-images.s-microsoft.com/image/apps.31997.13510798887167234.6cd52261-a276-49cf-9b6b-9ef8491fb799.30e70ce4-33c5-43d6-9af1-491fe4679377\" alt=\"Drawing\" style=\"width: 200px;\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Il progetto **InsTAP** ha come obiettivo quello di analizzare diversi profili Instagram e valutare la positività o negatività dei commenti presenti nei vari post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coded by Gianluca Di Franco e Rosario Amantia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Github Project](https://github.com/rosarioamantia/insTAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/tap_flow.png\" width=\"800\" height=\"800\"</center>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La sorgente è ovviamente il Social Network Instagram. Per recuperare i dati di cui avevamo bisogno, abbiamo fatto uso di un pacchetto chiamato **Instaloader**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/instaloader_ko.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Al fine di generare ed inviare i dati da Instagram abbiamo programmato un producer basato sul pacchetto instaloader citato in precedenza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Il producer si occuperà di recuperare ed inviare le seguenti informazioni contenute all'interno dei post degli utenti in esame: \n",
    "- utente che crea il post\n",
    "- Immagine \n",
    "- descrizione\n",
    "- data di creazione\n",
    "- coordinate\n",
    "- commenti "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "pip install instaloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "~~pip install instaloader~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "pip install --no-cache-dir git+https://github.com/IL2C/instaloader.git@login-fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "POSTS_LIMIT = int(os.getenv(\"POSTS_LIMIT\", '10'))\n",
    "COMMENTS_LIMIT = int(os.getenv(\"COMMENTS_LIMIT\", '20'))\n",
    "INSTA_USER = os.getenv(\"INSTA_USER\", \"<user_here>\")\n",
    "INSTA_PASS = os.getenv(\"INSTA_PASS\", \"<pass_here>\")\n",
    "USERS_TO_WATCH = os.getenv(\"USERS_TO_WATCH\", \"matteorenzi,giorgiameloni\").split(\",\")\n",
    "LOGSTASH_URL = \"http://logstash:9700\"\n",
    "\n",
    "insta = instaloader.Instaloader()\n",
    "insta.login(INSTA_USER, INSTA_PASS)\n",
    "for user in USERS_TO_WATCH:\n",
    "    posts = instaloader.Profile.from_username(insta.context, user).get_posts()\n",
    "\n",
    "    for i, post in enumerate(posts):\n",
    "        if i == POSTS_LIMIT:\n",
    "            break\n",
    "        comments = post.get_comments()\n",
    "        for index, comment in enumerate(comments):\n",
    "            if index == COMMENTS_LIMIT:\n",
    "                break\n",
    "\n",
    "            data = {\n",
    "                'message_id': index,\n",
    "                'post_id': f'{post.owner_username}_{i}',\n",
    "                'user': post.owner_username,\n",
    "                'comment': comment.text,\n",
    "                'caption': post.caption,\n",
    "                'image': post.url,\n",
    "                'timestamp': str(post.date_local),\n",
    "                'likes': post.likes,\n",
    "                'lat': post.location.lat if post.location else None,\n",
    "                'lng': post.location.lng if post.location else None\n",
    "            }\n",
    "            print(str(data))\n",
    "            sleep(1)\n",
    "            x = requests.post(LOGSTASH_URL, json=data, timeout=5)\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logstash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Logstash è uno strumento in grado di raccogliere, modificare e inoltrare dati provenienti da sorgenti diverse verso una o più destinazioni.\n",
    "Il file di configurazione di logstash è formato da tre sezioni:\n",
    "- **input**: ricevere/recuperare le informazioni;\n",
    "- **filter**: rimuovere/aggiungere/modificare campi;\n",
    "- **output**: inviare i dati alla destinazione. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/logstash.jpg\" width=\"1200\" height=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nel nostro caso, Logstash recupera i dati inviati dalla sorgente alla porta 9700 e li invia al broker di messaggi Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### il nostro .conf\n",
    "```\n",
    "input {\n",
    " http {\n",
    "    port => \"9700\"\n",
    "    host => \"0.0.0.0\"\n",
    "  }\n",
    "}\n",
    "\n",
    "filter {\n",
    "}\n",
    "\n",
    "output {\n",
    "  kafka {\n",
    "    codec => json\n",
    "    topic_id => \"instap\"\n",
    "    bootstrap_servers => \"http://broker:9092\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"images/logstash-explained.jpg\" width=\"350\" height=\"350\">\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![kafka.png](images/kafka.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Apache Kafka è una piattaforma distribuita basata sui messaggi e sul modello publish-subscribe con lo scopo di gestire streaming di dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Kafka è ideale per:\n",
    "- Scambi di dati affidabili tra componenti diversi\n",
    "- Gestione dei carichi di lavoro\n",
    "- Streaming in tempo reale per l'elaborazione dei dati\n",
    "- Supporto per la riproduzione di dati/messaggi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Concetti chiave di Kafka:\n",
    "- **Producer**: colui che invia i dati. Il producer definisce anche su quale topic dovrà arrivare il messaggio.\n",
    "- **Topic**: argomento del messaggio. \n",
    "    - i producers scelgono su quale topic devono inviare i messaggi;\n",
    "    - i consumers scelgono su quale topic leggere i messaggi. \n",
    "- **Partizione**: struttura dati all'interno del topic in cui vengono scritti i messaggi. Ogni topic ha 1 o più partizioni.\n",
    "- **Consumer**: colui che legge i messaggi. Ogni consumer appartiene ad un gruppo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"images/consumer-group-kafka.png\" width=\"1200\" height=\"1200\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All'interno del nostro progetto, Kafka è così suddiviso:\n",
    "- **Zookeper**: servizio centralizzatto per il mantenimento di configurazioni e denominazioni; \n",
    "- **Broker**: Cuore di Kafka, gestisce il flusso di dati proveniente da ElasticSearch;\n",
    "- **Init-Kafka**: Script per l'inizializzazione di topic, partition..\n",
    "- **Kafka-Ui**: interfaccia Web relativa a Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/spark-logo.png\" width=\"330px\" height=\"126px\"> &nbsp;</center>\n",
    "\n",
    "<p>Apache Spark è un framework open source per il calcolo distribuito che esegue le sue elaborazioni in memoria per migliorare le prestazioni delle applicazioni che analizzano Big Data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Si tratta di uno strumento estremamente veloce:\n",
    "- In memoria\n",
    "- Lazy\n",
    "- Semplicità di utilizzo\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"https://spark.apache.org/images/spark-stack.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>\n",
    "Per l'allenamento del modello è stato utilizzato PySpark ed in particolare la libreria Spark MLlib utilizzando la funzione \"LogisticRegression\" all'interno di una pipeline di machine learning.\n",
    "</p>\n",
    "\n",
    "```\n",
    "#tokenize the Instagram comments text    \n",
    "stage_1 = RegexTokenizer(inputCol= 'comment' , outputCol= 'tokens', pattern= '\\\\W')\n",
    "\n",
    "#remove the stop words\n",
    "stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')\n",
    "\n",
    "#create a word vector of size 50\n",
    "stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 50)\n",
    "\n",
    "#Logistic Regression Model\n",
    "model = LogisticRegression(featuresCol= 'vector', labelCol= 'positive')\n",
    "\n",
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])\n",
    "\n",
    "dataset = spark.read.csv('../tap/spark/dataset/social_training_set.csv',\n",
    "                         schema=training_set_schema,\n",
    "                         header=True,\n",
    "                         sep=',')\n",
    "\n",
    "#trained model\n",
    "pipelineFit = pipeline.fit(dataset)\n",
    "\n",
    "```\n",
    "\n",
    "quando avviene la chiamata pipeline.fit(), viene adattato il modello al training set di input e gli stage vengono eseguiti in ordine per ogni record del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"images/before_training.jpg\" alighn=\"center\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"images/after_training_1.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Una volta ottenuto il modello allenato, il passo successivo è quello di leggere i dati inseriti in tempo reale da Kafka nel topic \"instap_topic\".\n",
    "<br>\n",
    "<br>\n",
    "Per leggere il flusso di dati viene usata la libreria Spark Structured Streaming, una libreria per processare i dati in streaming all'interno della libreria Spark SQL\n",
    "```\n",
    "\n",
    "#Create DataFrame representing the stream of input lines from Kafka\n",
    "df = spark.readStream.format('kafka')\n",
    "    .option('kafka.bootstrap.servers', kafkaServer) \\\n",
    "        .option('subscribe', \"instap_topic\"). \\\n",
    "            option(\"startingOffsets\",\"earliest\").load()\n",
    "```\n",
    "<center><img src=\"images/kafka_spark.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p>\n",
    "   A questo punto, tramite un Output Structured Streaming, si elaborano i dati in stream applicando il modello sui dati per ogni micro-batch ed effettuando una predizione sul commento che verrà aggiunta in una colonna \"prediction\" nel DataFrame risultante<br>\n",
    "</p>\n",
    "\n",
    "```\n",
    "df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n",
    "```\n",
    "<center><img src=\"images/spark_ml.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Una volta processati e analizzati, i dati vengono inviati da PySpark verso Elasticsearch all'interno dell'index \"instap_index\" tramite il client Python\n",
    "\n",
    "```\n",
    "es = Elasticsearch(hosts=elastic_host, verify_certs = False)\n",
    "resp = es.index(index = \"instap_index\", id = id, document = alayzed_data)\n",
    "\n",
    "```\n",
    "<br>\n",
    "<img src=\"images/spark_es.png\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/elasticsearch.png\" align=\"center\" width=\"400px\" height=\"400px\"></center>\n",
    "<p>\n",
    "    Elasticsearch è un motore per la ricerca e l'analisi di dati: è in grado di gestire tutte le tipologie di dato (testuale, numerico, geospaziale, strutturato e non strutturato) ed è conosciuto per la sua natura distribuita, velocità e scalabilità.\n",
    "<br>\n",
    "<br>\n",
    "Elasticsearch è stato utilizzato per indicizzare e memorizzare i dati precedentemente elaborati e inviarli a Kibana per visualizzarli.\n",
    "</p>\n",
    "<b>Nota: </b>una volta memorizzati, i dati  possono essere recuperati in modo veloce ed efficiente attraverso semplici chiamate REST o le apposite API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p><b>Le conferme che rendono felici<b></p>\n",
    "<img src=\"images/es_log.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/kibana.png\" width=\"430px\" height=\"226px\">\n",
    "\n",
    "Kibana è una UI utilizzata per visualizzare ed analizzare i dati di Elasticsearch con l'ausilio di grafici.\n",
    "<br>\n",
    "<br>\n",
    "Di seguito vengono riportati alcuni grafici mostrati nella dashboard. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"images/Homer-kibana.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![kibana-dashboard.jpg](images/all-dashboard.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![kibana-post-prediction.jpg](images/kibana-post-prediction.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![kibana-donut.jpg](images/kibana-donut.jpg)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "48a4a3db1827b9653c65b873d88ab4a1a166ed0731694d015661779b08759ad9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
